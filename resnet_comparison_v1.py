# -*- coding: utf-8 -*-
"""Resnet_Comparison_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJ9zUwQWFefFDMjIDz6WDzH9zC4HJb-3
"""

print("started")
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet50
from torchvision.models import resnet18
from torchvision.datasets import CIFAR100
from sklearn.metrics import precision_recall_fscore_support
import numpy as np
import matplotlib.pyplot as plt
# Define the transformation for data preprocessing
transform_train = transforms.Compose(
    [
        transforms.RandomHorizontalFlip(),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)

transform_test = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train
)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, shuffle=True, num_workers=2
)

testset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test
)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=128, shuffle=False, num_workers=2
)

# Load ResNet-50 model
model = resnet50(pretrained=False, num_classes=10)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)

# Move model to GPU if available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training loop
num_epochs = 200
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    # Update learning rate
    scheduler.step()

    # Print training progress
    print(
        f"Epoch [{epoch + 1}/{num_epochs}] Loss: {train_loss / (batch_idx + 1):.4f} "
        f"Acc: {100.0 * correct / total:.2f}%"
    )

# Evaluate on test set
model.eval()
test_loss = 0.0
correct = 0
total = 0

with torch.no_grad():
    for batch_idx, (inputs, targets) in enumerate(testloader):
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        test_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
#Print test set accuracy
print(f"Test set: Average loss: {test_loss / (batch_idx + 1):.4f}, Accuracy: {100.0 * correct / total:.2f}%")

# Load CIFAR-100 dataset
trainset_cifar100 = CIFAR100(root='./data', train=True, download=True, transform=transform_train)
trainloader_cifar100 = torch.utils.data.DataLoader(trainset_cifar100, batch_size=128, shuffle=True, num_workers=2)

testset_cifar100 = CIFAR100(root='./data', train=False, download=True, transform=transform_test)
testloader_cifar100 = torch.utils.data.DataLoader(testset_cifar100, batch_size=128, shuffle=False, num_workers=2)

# Shrink ResNet-50 to ResNet-18
resnet18_pretrained = nn.Sequential(*list(model.children())[:-3])
resnet18_model = resnet18(pretrained=False, num_classes=100)
resnet18_model.load_state_dict(resnet18_pretrained.state_dict(), strict=False)
resnet18_model.to(device)

# Fine-tune ResNet-18 on CIFAR-100
optimizer_ft = optim.SGD(resnet18_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler_ft = optim.lr_scheduler.MultiStepLR(optimizer_ft, milestones=[100, 150], gamma=0.1)

print("Fine-tuning ResNet-18 on CIFAR-100:")
for epoch in range(num_epochs):
    resnet18_model.train()
    train_loss = 0.0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in enumerate(trainloader_cifar100):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer_ft.zero_grad()
        outputs = resnet18_model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer_ft.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    scheduler_ft.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}] Loss: {train_loss / (batch_idx + 1):.4f} Acc: {100.0 * correct / total:.2f}%")

# Train a new ResNet-18 from scratch on CIFAR-100
resnet18_scratch = resnet18(pretrained=False, num_classes=100).to(device)
optimizer_scratch = optim.SGD(resnet18_scratch.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler_scratch = optim.lr_scheduler.MultiStepLR(optimizer_scratch, milestones=[100, 150], gamma=0.1)

print("Training ResNet-18 from scratch on CIFAR-100:")
for epoch in range(num_epochs):
    resnet18_scratch.train()
    train_loss = 0.0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in enumerate(trainloader_cifar100):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer_scratch.zero_grad()
        outputs = resnet18_scratch(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer_scratch.step()

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    scheduler_scratch.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}] Loss: {train_loss / (batch_idx + 1):.4f} Acc: {100.0 * correct / total:.2f}%")

# Evaluate the fine-tuned ResNet-18 model on the CIFAR-100 test set
resnet18_model.eval()
test_loss = 0.0
correct = 0
total = 0

with torch.no_grad():
    for batch_idx, (inputs, targets) in enumerate(testloader_cifar100):
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = resnet18_model(inputs)
        loss = criterion(outputs, targets)

        test_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

print(f"Fine-tuned ResNet-18: Test set: Average loss: {test_loss / (batch_idx + 1):.4f}, Accuracy: {100.0 * correct / total:.2f}%")

# Evaluate the ResNet-18 model trained from scratch on the CIFAR-100 test set
resnet18_scratch.eval()
test_loss = 0.0
correct = 0
total = 0

with torch.no_grad():
    for batch_idx, (inputs, targets) in enumerate(testloader_cifar100):
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = resnet18_scratch(inputs)
        loss = criterion(outputs, targets)

        test_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

print(f"ResNet-18 from scratch: Test set: Average loss: {test_loss / (batch_idx + 1):.4f}, Accuracy: {100.0 * correct / total:.2f}%")

def evaluate_model(model, dataloader):
    model.eval()
    all_targets = []
    all_predictions = []

    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_targets.extend(targets.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    return precision_recall_fscore_support(all_targets, all_predictions, average=None)

# Evaluate fine-tuned ResNet-18
precision_ft, recall_ft, f1_score_ft, _ = evaluate_model(resnet18_model, testloader_cifar100)

# Evaluate ResNet-18 trained from scratch
precision_scratch, recall_scratch, f1_score_scratch, _ = evaluate_model(resnet18_scratch, testloader_cifar100)

# Print results
print("Class | Fine-tuned | From scratch")
print("------+------------+-------------")
for i in range(len(precision_ft)):
    print(f"{i:4d} | {f1_score_ft[i]:.4f}    | {f1_score_scratch[i]:.4f}")

# Function to train and evaluate a model
def train_and_evaluate(model, trainloader, testloader, optimizer, scheduler, num_epochs):
    train_losses = []
    train_accuracies = []
    test_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0
        for batch_idx, (inputs, targets) in enumerate(trainloader):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        scheduler.step()
        train_losses.append(train_loss / (batch_idx + 1))
        train_accuracies.append(100.0 * correct / total)

        test_acc = evaluate_accuracy(model, testloader)
        test_accuracies.append(test_acc)

    return train_losses, train_accuracies, test_accuracies

# Function to evaluate the accuracy of a model
def evaluate_accuracy(model, dataloader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    return 100.0 * correct / total

# Train and evaluate both models
train_losses_ft, train_accuracies_ft, test_accuracies_ft = train_and_evaluate(resnet18_model, trainloader_cifar100, testloader_cifar100, optimizer_ft, scheduler_ft, num_epochs)
train_losses_scratch, train_accuracies_scratch, test_accuracies_scratch = train_and_evaluate(resnet18_scratch, trainloader_cifar100, testloader_cifar100, optimizer_scratch, scheduler_scratch, num_epochs)

# Save training loss plot
print("training_loss_comparison.png")
plt.figure()
plt.plot(train_losses_ft, label='Fine-tuned ResNet-18')
plt.plot(train_losses_scratch, label='ResNet-18 from scratch')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.legend()
plt.title("Training Loss Comparison")
plt.savefig('training_loss_comparison1.png')
plt.close()
# Save training accuracy plot
print('training_accuracy_comparison.png')
plt.figure()
plt.plot(train_accuracies_ft, label='Fine-tuned ResNet-18')
plt.plot(train_accuracies_scratch, label='ResNet-18 from scratch')
plt.xlabel('Epoch')
plt.ylabel('Training Accuracy')
plt.legend()
plt.title("Training Accuracy Comparison")
plt.savefig('training_accuracy_comparison1.png')
plt.close()
# Save test accuracy plot
print("test")
plt.figure()
plt.plot(test_accuracies_ft, label='Fine-tuned ResNet-18')
plt.plot(test_accuracies_scratch, label='ResNet-18 from scratch')
plt.xlabel('Epoch')
plt.ylabel('Test Accuracy')
plt.legend()
plt.title("Test Accuracy Comparison")
plt.savefig('test_accuracy_comparison1.png')
plt.close()